{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8453d741-894f-4edc-8efc-4e109844cb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Deequ - Data Validation and Data Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8b141aa-1357-4533-8dd3-7a7facde4d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Installation](#installation)\n",
    "- [Setup](#setup)\n",
    "- [DataLoader](#dataloader)\n",
    "- [Analyzers](#analyzers)\n",
    "- [Data Profling](data-profiling)\n",
    "- [Constraint Suggestion](#constraint-suggestion)\n",
    "- [Constraint Verification](#constraint-verification)\n",
    "- [Metrics Repositories](#metrics-repositories)\n",
    "- [Anomaly Detection](#anomaly-detetcion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "917329e6-5e0e-44f5-bf1d-97fc664cb8b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Deequ is a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets.\n",
    "\n",
    "**Purpose:**\n",
    "- Deequ's purpose is to \"unit-test\" data to find errors early, before the data gets fed to consuming systems or machine learning algorithms\n",
    "\n",
    "**Capabilities:**\n",
    "- Suitable for large data files\n",
    "- It works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. \n",
    "- In deequ we can explicitly state our assumptions about underlying in the form of a \"unit-test\" for data, which can be verified on a piece of data at hand. If the data has errors, we can \"quarantine\" and fix it, before we feed it to an application.\n",
    "- **PyDeequ** Python API for Deequ is avaiable which can be used in Pyspark env.\n",
    "\n",
    "**Components:**\n",
    "\n",
    "There are 4 main components of Deequ\n",
    "- Metrics Computation:\n",
    "    - `Profiles` leverages Analyzers to analyze each column of a dataset.\n",
    "    - `Analyzers` serve here as a foundational module that computes metrics for data profiling and validation at scale.\n",
    "- Constraint Suggestion:\n",
    "    - Specify rules for various groups of Analyzers to be run over a dataset to return back a collection of constraints suggested to run in a Verification Suite.\n",
    "- Constraint Verification:\n",
    "    - Perform data validation on a dataset with respect to various constraints set by users.\n",
    "    - Also support pattern matching and pattern(regex) related constraints\n",
    "- Metrics Repository\n",
    "    - Allows for persistence and tracking of Deequ runs over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a7066e1-fb7a-4925-9905-ebffbe0dda67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672cbf36-a4ce-42b7-9fa8-b7ff73a0e9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[Github - Source Code](https://github.com/awslabs/python-deequ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4866d8f-c290-4b88-9362-6c00e723e277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that we need to install following maven library to cluster itself inorder to intialize spark session with deequ dependencies `com.amazon.deequ:deequ:2.0.8-spark-3.5`. Available via [maven central](http://mvnrepository.com/artifact/com.amazon.deequ/deequ).\n",
    "\n",
    "Here is a snippet showing how to install the above dependency using cluster configuration. The version of artifact depends on your spark version installed on cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3f47e4-4733-48f8-b8f8-687b217e6bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d748cdc-3dba-4557-8cb3-1323011a9253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting pydeequ\n",
      "  Downloading pydeequ-1.4.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /databricks/python3/lib/python3.10/site-packages (from pydeequ) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /databricks/python3/lib/python3.10/site-packages (from pydeequ) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=0.23.0->pydeequ) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.23.0->pydeequ) (1.16.0)\n",
      "Installing collected packages: pydeequ\n",
      "Successfully installed pydeequ-1.4.0\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51df71d3-3b45-49d3-9734-9adac1bd8075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "#set the spark version installed on cluster\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd8bd6a-9050-47ec-828d-72140d450e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab57447-599d-4e12-92b6-6048bd89df24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", 'com.amazon.deequ:deequ:2.0.8-spark-3.5')\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b255a59f-d371-4c7b-982b-5ffc13f163be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Additional Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989b6f7a-366c-4935-b588-476d05e8c4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import *\n",
    "from pydeequ.profiles import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62568f54-e61b-4f94-a8e2-f7a94cecc59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'sample_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0313e1a3-5c4e-4c97-9ab5-794d22f57873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * from {table_name} where current_date = '2024-12-01'\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908e633e-07b3-4404-8a97-00a3fb341c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_cols = [\"user_id\", \"lifetime_order_count\", \"lifetime_gsv\", \"lifetime_gppo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db294154-e283-483a-aabf-ffa6c5ba5ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Analyzers\n",
    "\n",
    "All avaialble methods - [Analyzers](https://github.com/awslabs/python-deequ/blob/master/docs/analyzers.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de49d6b8-4329-459a-9a6b-f760fc3518d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:159: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>entity</th><th>instance</th><th>name</th><th>value</th></tr></thead><tbody><tr><td>Column</td><td>user_id</td><td>Distinctness</td><td>1.0</td></tr><tr><td>Column</td><td>user_id</td><td>ApproxCountDistinct</td><td>2.2857264E7</td></tr><tr><td>Column</td><td>top users</td><td>Compliance</td><td>0.60715486774642</td></tr><tr><td>Column</td><td>user_id</td><td>Completeness</td><td>1.0</td></tr><tr><td>Column</td><td>user_id</td><td>MinLength</td><td>36.0</td></tr><tr><td>Column</td><td>user_id</td><td>MaxLength</td><td>36.0</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>StandardDeviation</td><td>9756.243642500609</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Sum</td><td>7.51354820868327E10</td></tr><tr><td>Dataset</td><td>*</td><td>Size</td><td>2.0832782E7</td></tr><tr><td>Column</td><td>lifetime_order_count</td><td>Distinctness</td><td>3.403290064668272E-5</td></tr><tr><td>Multicolumn</td><td>lifetime_gsv,lifetime_gppo</td><td>Correlation</td><td>0.9647748973569691</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Maximum</td><td>1274423.0</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Minimum</td><td>-2720.0</td></tr><tr><td>Column</td><td>lifetime_gppo</td><td>Maximum</td><td>186804.4792057906</td></tr><tr><td>Column</td><td>lifetime_gppo</td><td>Minimum</td><td>-47596.31383520645</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Column",
         "user_id",
         "Distinctness",
         1
        ],
        [
         "Column",
         "user_id",
         "ApproxCountDistinct",
         22857264
        ],
        [
         "Column",
         "top users",
         "Compliance",
         0.60715486774642
        ],
        [
         "Column",
         "user_id",
         "Completeness",
         1
        ],
        [
         "Column",
         "user_id",
         "MinLength",
         36
        ],
        [
         "Column",
         "user_id",
         "MaxLength",
         36
        ],
        [
         "Column",
         "lifetime_gsv",
         "StandardDeviation",
         9756.243642500609
        ],
        [
         "Column",
         "lifetime_gsv",
         "Sum",
         75135482086.8327
        ],
        [
         "Dataset",
         "*",
         "Size",
         20832782
        ],
        [
         "Column",
         "lifetime_order_count",
         "Distinctness",
         0.00003403290064668272
        ],
        [
         "Multicolumn",
         "lifetime_gsv,lifetime_gppo",
         "Correlation",
         0.9647748973569691
        ],
        [
         "Column",
         "lifetime_gsv",
         "Maximum",
         1274423
        ],
        [
         "Column",
         "lifetime_gsv",
         "Minimum",
         -2720
        ],
        [
         "Column",
         "lifetime_gppo",
         "Maximum",
         186804.4792057906
        ],
        [
         "Column",
         "lifetime_gppo",
         "Minimum",
         -47596.31383520645
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "entity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "instance",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df.select(test_cols)) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Correlation(\"lifetime_gsv\", \"lifetime_gppo\")) \\\n",
    "                    .addAnalyzer(Completeness(\"user_id\")) \\\n",
    "                    .addAnalyzer(MinLength(\"user_id\")) \\\n",
    "                    .addAnalyzer(MaxLength(\"user_id\")) \\\n",
    "                    .addAnalyzer(Maximum(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Maximum(\"lifetime_gppo\")) \\\n",
    "                    .addAnalyzer(Minimum(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Minimum(\"lifetime_gppo\")) \\\n",
    "                    .addAnalyzer(Distinctness(\"lifetime_order_count\")) \\\n",
    "                    .addAnalyzer(Distinctness(\"user_id\")) \\\n",
    "                    .addAnalyzer(StandardDeviation(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Sum(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Compliance(\"top users\", \"lifetime_gppo >= 0\")) \\\n",
    "                    .addAnalyzer(ApproxCountDistinct(\"user_id\")) \\\n",
    "                    .run()\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aa58b66-1577-4b3f-8aa6-314be8f225e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can convert above df into pandas too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e7a0eb-650d-4a77-8184-1897760840dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:159: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>user_id</td>\n",
       "      <td>Distinctness</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>user_id</td>\n",
       "      <td>ApproxCountDistinct</td>\n",
       "      <td>2.285726e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Column</td>\n",
       "      <td>top users</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>6.071549e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>user_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>user_id</td>\n",
       "      <td>MinLength</td>\n",
       "      <td>3.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>user_id</td>\n",
       "      <td>MaxLength</td>\n",
       "      <td>3.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_gsv</td>\n",
       "      <td>StandardDeviation</td>\n",
       "      <td>9.756244e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_gsv</td>\n",
       "      <td>Sum</td>\n",
       "      <td>7.513548e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>2.083278e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_order_count</td>\n",
       "      <td>Distinctness</td>\n",
       "      <td>3.403290e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Multicolumn</td>\n",
       "      <td>lifetime_gsv,lifetime_gppo</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>9.647749e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_gsv</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>1.274423e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_gsv</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>-2.720000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_gppo</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>1.868045e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Column</td>\n",
       "      <td>lifetime_gppo</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>-4.759631e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         entity                    instance                 name         value\n",
       "0        Column                     user_id         Distinctness  1.000000e+00\n",
       "1        Column                     user_id  ApproxCountDistinct  2.285726e+07\n",
       "2        Column                   top users           Compliance  6.071549e-01\n",
       "3        Column                     user_id         Completeness  1.000000e+00\n",
       "4        Column                     user_id            MinLength  3.600000e+01\n",
       "5        Column                     user_id            MaxLength  3.600000e+01\n",
       "6        Column                lifetime_gsv    StandardDeviation  9.756244e+03\n",
       "7        Column                lifetime_gsv                  Sum  7.513548e+10\n",
       "8       Dataset                           *                 Size  2.083278e+07\n",
       "9        Column        lifetime_order_count         Distinctness  3.403290e-05\n",
       "10  Multicolumn  lifetime_gsv,lifetime_gppo          Correlation  9.647749e-01\n",
       "11       Column                lifetime_gsv              Maximum  1.274423e+06\n",
       "12       Column                lifetime_gsv              Minimum -2.720000e+03\n",
       "13       Column               lifetime_gppo              Maximum  1.868045e+05\n",
       "14       Column               lifetime_gppo              Minimum -4.759631e+04"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysisResult_pandas_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult, pandas=True)\n",
    "analysisResult_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d67746a-5bae-46f1-ac01-8d29dfde8fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Profiling\n",
    "\n",
    "[All functionalities available for profiling](https://github.com/awslabs/python-deequ/blob/48ed4420aa648a71ea34c75d39d0cc829f98abda/docs/profiles.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af4d1c1-274a-4a36-a185-75085f55f771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.profiles import *\n",
    "\n",
    "result = ColumnProfilerRunner(spark) \\\n",
    "    .onData(df.select(test_cols)) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f584ec-061f-4f47-a735-9b3fd70bbc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_id', 'lifetime_order_count', 'lifetime_gsv', 'lifetime_gppo'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.profiles.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1053221b-c962-4381-8dde-d49d26f59c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of 'lifetime_gsv':\n",
      "\t minimum: -2720.0\n",
      "\t maximum: 1274423.0\n",
      "\t mean: 3606.5985851929963\n",
      "\t standard deviation: 9756.243642500594\n"
     ]
    }
   ],
   "source": [
    "lifetimegsv_profile = result.profiles['lifetime_gsv']\n",
    "\n",
    "print(f'Statistics of \\'lifetime_gsv\\':')\n",
    "print('\\t',f\"minimum: {lifetimegsv_profile.minimum}\")\n",
    "print('\\t',f\"maximum: {lifetimegsv_profile.maximum}\")\n",
    "print('\\t',f\"mean: {lifetimegsv_profile.mean}\")\n",
    "print('\\t',f\"standard deviation: {lifetimegsv_profile.stdDev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477e8815-4e50-47c2-90b3-ec586a915c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': <pydeequ.profiles.StandardColumnProfile at 0xffff20dc3160>,\n",
       " 'lifetime_order_count': <pydeequ.profiles.NumericColumnProfile at 0xffff20dc16f0>,\n",
       " 'lifetime_gsv': <pydeequ.profiles.NumericColumnProfile at 0xffff2096bcd0>,\n",
       " 'lifetime_gppo': <pydeequ.profiles.NumericColumnProfile at 0xffff20968eb0>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f9d20a-8edd-4d34-9861-ab634bfdc772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:447)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1311)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1028)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:74)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:74)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:74)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:74)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:989)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:799)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:825)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:824)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:879)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:672)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:447)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1311)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:1028)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:74)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:74)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:74)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:74)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:989)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:799)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$5(Chauffeur.scala:825)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:824)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:879)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:672)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:669)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:687)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)",
        "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:664)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:147)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1020)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:941)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:545)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:514)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$4(ActivityContextFactory.scala:405)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:58)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$1(ActivityContextFactory.scala:405)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:380)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:159)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:514)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:404)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)",
        "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:104)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:47)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:86)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col, profile in result.profiles.items():\n",
    "    print(f'Column: {col}')\n",
    "    \n",
    "    if isinstance(profile, pydeequ.profiles.NumericColumnProfile):  \n",
    "        d = {}\n",
    "        d['minimum'] = profile.minimum\n",
    "        d['maximum'] = profile.maximum\n",
    "        d['mean'] = profile.mean\n",
    "        d['standard_deviation'] = profile.stdDev\n",
    "        d['distribution'] = {}\n",
    "        d['distribution']['KLL'] = {}\n",
    "        d['distribution']['KLL']['buckets'] = {}\n",
    "        for b in range(len(profile.kll.buckets)): \n",
    "            d['distribution']['KLL']['buckets'][f'bucket_{b}'] = {\n",
    "                'lowValue': profile.kll.buckets[b].lowValue,\n",
    "                'highValue':profile.kll.buckets[b].highValue,\n",
    "                'count': profile.kll.buckets[b].count\n",
    "            }\n",
    "        d['distribution']['KLL']['sketch'] = {\n",
    "            'c': profile.kll.parameters[0],\n",
    "            'k': profile.kll.parameters[1]\n",
    "        }\n",
    "        d['distribution']['KLL']['data'] = profile.kll.data\n",
    "\n",
    "        print(json.dumps(d, indent=2))\n",
    " \n",
    "    else: \n",
    "        for i in profile.histogram: \n",
    "            print(f\"{i.value} occurred {i.count} times (ratio is: {i.ratio})\")\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49efd3c7-61ff-417c-8044-4b6c70ec81fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Constraint Suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3865a117-4e1d-41d7-abe8-6b63e1d0d023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Constraint suggestion functionality into deequ to assist users in finding reasonable constraints for users data.\n",
    "\n",
    "[All avaialble constraint suggestions](https://github.com/awslabs/python-deequ/blob/48ed4420aa648a71ea34c75d39d0cc829f98abda/docs/suggestions.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84bf623-079d-4b91-92cd-44a5c3eb49d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.suggestions import *\n",
    "\n",
    "suggestionResult = ConstraintSuggestionRunner(spark) \\\n",
    "             .onData(df.select(test_cols)) \\\n",
    "             .addConstraintRule(CompleteIfCompleteRule()) \\\n",
    "             .addConstraintRule(NonNegativeNumbersRule()) \\\n",
    "             .addConstraintRule(RetainTypeRule()) \\\n",
    "             .addConstraintRule(UniqueIfApproximatelyUniqueRule()) \\\n",
    "             .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2499cf12-bf64-4bd4-a6bc-4f3b137625b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint suggestion for 'user_id': 'user_id' is not null\n",
      "The corresponding Python code is: .isComplete(\"user_id\")\n",
      "\n",
      "Constraint suggestion for 'lifetime_order_count': 'lifetime_order_count' is not null\n",
      "The corresponding Python code is: .isComplete(\"lifetime_order_count\")\n",
      "\n",
      "Constraint suggestion for 'lifetime_order_count': 'lifetime_order_count' has no negative values\n",
      "The corresponding Python code is: .isNonNegative(\"lifetime_order_count\")\n",
      "\n",
      "Constraint suggestion for 'lifetime_gsv': 'lifetime_gsv' is not null\n",
      "The corresponding Python code is: .isComplete(\"lifetime_gsv\")\n",
      "\n",
      "Constraint suggestion for 'lifetime_gppo': 'lifetime_gppo' is not null\n",
      "The corresponding Python code is: .isComplete(\"lifetime_gppo\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sugg in suggestionResult['constraint_suggestions']:\n",
    "    print(f\"Constraint suggestion for \\'{sugg['column_name']}\\': {sugg['description']}\")\n",
    "    print(f\"The corresponding Python code is: {sugg['code_for_constraint']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0029701-9b23-4790-9c4d-38d67e702237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Constraint Verification\n",
    "\n",
    "[All available constraints](https://github.com/awslabs/python-deequ/blob/48ed4420aa648a71ea34c75d39d0cc829f98abda/docs/checks.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec6220-5e72-4e7e-861f-97be1301a547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Run Status: Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:159: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>check</th><th>check_level</th><th>check_status</th><th>constraint</th><th>constraint_status</th><th>constraint_message</th></tr></thead><tbody><tr><td>test_check</td><td>Error</td><td>Error</td><td>SizeConstraint(Size(None))</td><td>Success</td><td></td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>MinimumConstraint(Minimum(lifetime_gppo,None,None))</td><td>Success</td><td></td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>MaximumConstraint(Maximum(lifetime_gppo,None,None))</td><td>Success</td><td></td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>CompletenessConstraint(Completeness(user_id,None,None))</td><td>Success</td><td></td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>UniquenessConstraint(Uniqueness(List(user_id),None,None))</td><td>Success</td><td></td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>UniquenessConstraint(Uniqueness(List(lifetime_order_count),None,None))</td><td>Failure</td><td>Value: 5.232138463312293E-6 does not meet the constraint requirement!</td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>ComplianceConstraint(Compliance(lifetime_gppo is non-negative,COALESCE(CAST(lifetime_gppo AS DECIMAL(20,10)), 0.0) >= 0,None,List(lifetime_gppo),None))</td><td>Failure</td><td>Value: 0.60715486774642 does not meet the constraint requirement!</td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>ComplianceConstraint(Compliance(lifetime_gsv is non-negative,COALESCE(CAST(lifetime_gsv AS DECIMAL(20,10)), 0.0) >= 0,None,List(lifetime_gsv),None))</td><td>Failure</td><td>Value: 0.9999991359771345 does not meet the constraint requirement!</td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>ComplianceConstraint(Compliance(lifetime_gsv is greater than lifetime_gppo,lifetime_gsv > lifetime_gppo,None,List(lifetime_gsv, lifetime_gppo),None))</td><td>Failure</td><td>Value: 0.9965364683410982 does not meet the constraint requirement!</td></tr><tr><td>test_check</td><td>Error</td><td>Error</td><td>PatternMatchConstraint(user_id, ba(r|z))</td><td>Success</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "test_check",
         "Error",
         "Error",
         "SizeConstraint(Size(None))",
         "Success",
         ""
        ],
        [
         "test_check",
         "Error",
         "Error",
         "MinimumConstraint(Minimum(lifetime_gppo,None,None))",
         "Success",
         ""
        ],
        [
         "test_check",
         "Error",
         "Error",
         "MaximumConstraint(Maximum(lifetime_gppo,None,None))",
         "Success",
         ""
        ],
        [
         "test_check",
         "Error",
         "Error",
         "CompletenessConstraint(Completeness(user_id,None,None))",
         "Success",
         ""
        ],
        [
         "test_check",
         "Error",
         "Error",
         "UniquenessConstraint(Uniqueness(List(user_id),None,None))",
         "Success",
         ""
        ],
        [
         "test_check",
         "Error",
         "Error",
         "UniquenessConstraint(Uniqueness(List(lifetime_order_count),None,None))",
         "Failure",
         "Value: 5.232138463312293E-6 does not meet the constraint requirement!"
        ],
        [
         "test_check",
         "Error",
         "Error",
         "ComplianceConstraint(Compliance(lifetime_gppo is non-negative,COALESCE(CAST(lifetime_gppo AS DECIMAL(20,10)), 0.0) >= 0,None,List(lifetime_gppo),None))",
         "Failure",
         "Value: 0.60715486774642 does not meet the constraint requirement!"
        ],
        [
         "test_check",
         "Error",
         "Error",
         "ComplianceConstraint(Compliance(lifetime_gsv is non-negative,COALESCE(CAST(lifetime_gsv AS DECIMAL(20,10)), 0.0) >= 0,None,List(lifetime_gsv),None))",
         "Failure",
         "Value: 0.9999991359771345 does not meet the constraint requirement!"
        ],
        [
         "test_check",
         "Error",
         "Error",
         "ComplianceConstraint(Compliance(lifetime_gsv is greater than lifetime_gppo,lifetime_gsv > lifetime_gppo,None,List(lifetime_gsv, lifetime_gppo),None))",
         "Failure",
         "Value: 0.9965364683410982 does not meet the constraint requirement!"
        ],
        [
         "test_check",
         "Error",
         "Error",
         "PatternMatchConstraint(user_id, ba(r|z))",
         "Success",
         ""
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "check",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "check_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "check_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "constraint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "constraint_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "constraint_message",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pydeequ.verification import *\n",
    "\n",
    "check = Check(spark, CheckLevel.Error, \"test_check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df.select(test_cols)) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 3000000) \\\n",
    "        .hasMin(\"lifetime_gppo\", lambda x: x < 0) \\\n",
    "        .hasMax(\"lifetime_gppo\", lambda x: x > 0)  \\\n",
    "        .isComplete(\"user_id\")  \\\n",
    "        .isUnique(\"user_id\")  \\\n",
    "        .isUnique(\"lifetime_order_count\")  \\\n",
    "        .isNonNegative(\"lifetime_gppo\") \\\n",
    "        .isNonNegative(\"lifetime_gsv\") \\\n",
    "        .isGreaterThan(\"lifetime_gsv\", \"lifetime_gppo\") \\\n",
    "        .hasPattern(column='user_id', pattern=r\"ba(r|z)\", assertion=lambda x: x == 0/3)\n",
    "    ) \\\n",
    "    .run()\n",
    "\n",
    "print(f\"Verification Run Status: {checkResult.status}\")\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult) #checkResultsAsJson also available to get results in json format\n",
    "display(checkResult_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e49dad-ac45-4516-8e84-539073ab78ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Metrics Repository\n",
    "\n",
    "PyDeequ allows us to persist the metrics we computed on dataframes in a so-called MetricsRepository.\n",
    "\n",
    "**Metrics Repository allows us to store the metrics in json format on the local disk (note that it also supports HDFS and S3).**\n",
    "\n",
    "More about metrics repository \n",
    "- [Metrics Repository](https://github.com/awslabs/python-deequ/blob/48ed4420aa648a71ea34c75d39d0cc829f98abda/tutorials/repository.ipynb)\n",
    "- [Metrics Repository - DBFS](https://github.com/awslabs/python-deequ/blob/48ed4420aa648a71ea34c75d39d0cc829f98abda/tutorials/repository_file_dbfs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b65a279-df59-45c2-934d-49bfd5ff57ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics_file path: /local_disk0/tmp/1733472396389-0/metrics.json\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.repository import *\n",
    "\n",
    "metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'metrics.json')\n",
    "print(f'metrics_file path: {metrics_file}')\n",
    "repository = FileSystemMetricsRepository(spark, metrics_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ecd87f-2856-46de-b85c-bcc612837fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Each set of metrics that we computed needs be indexed by a so-called ResultKey, which contains a timestamp and supports arbitrary tags in the form of key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598560df-3034-4eaa-977f-02eabbe35dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "key_tags = {'tag': 'gp-data-test'}\n",
    "resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0afc1ca-5216-43b3-9aaf-8a9bfb911a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:159: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>entity</th><th>instance</th><th>name</th><th>value</th></tr></thead><tbody><tr><td>Column</td><td>user_id</td><td>Distinctness</td><td>1.0</td></tr><tr><td>Column</td><td>user_id</td><td>ApproxCountDistinct</td><td>2.2857264E7</td></tr><tr><td>Column</td><td>top users</td><td>Compliance</td><td>0.60715486774642</td></tr><tr><td>Column</td><td>user_id</td><td>Completeness</td><td>1.0</td></tr><tr><td>Column</td><td>user_id</td><td>MinLength</td><td>36.0</td></tr><tr><td>Column</td><td>user_id</td><td>MaxLength</td><td>36.0</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>StandardDeviation</td><td>9756.243642500609</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Sum</td><td>7.51354820868327E10</td></tr><tr><td>Dataset</td><td>*</td><td>Size</td><td>2.0832782E7</td></tr><tr><td>Column</td><td>lifetime_order_count</td><td>Distinctness</td><td>3.403290064668272E-5</td></tr><tr><td>Multicolumn</td><td>lifetime_gsv,lifetime_gppo</td><td>Correlation</td><td>0.9647748973569691</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Maximum</td><td>1274423.0</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Minimum</td><td>-2720.0</td></tr><tr><td>Column</td><td>lifetime_gppo</td><td>Maximum</td><td>186804.4792057906</td></tr><tr><td>Column</td><td>lifetime_gppo</td><td>Minimum</td><td>-47596.31383520645</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Column",
         "user_id",
         "Distinctness",
         1
        ],
        [
         "Column",
         "user_id",
         "ApproxCountDistinct",
         22857264
        ],
        [
         "Column",
         "top users",
         "Compliance",
         0.60715486774642
        ],
        [
         "Column",
         "user_id",
         "Completeness",
         1
        ],
        [
         "Column",
         "user_id",
         "MinLength",
         36
        ],
        [
         "Column",
         "user_id",
         "MaxLength",
         36
        ],
        [
         "Column",
         "lifetime_gsv",
         "StandardDeviation",
         9756.243642500609
        ],
        [
         "Column",
         "lifetime_gsv",
         "Sum",
         75135482086.8327
        ],
        [
         "Dataset",
         "*",
         "Size",
         20832782
        ],
        [
         "Column",
         "lifetime_order_count",
         "Distinctness",
         0.00003403290064668272
        ],
        [
         "Multicolumn",
         "lifetime_gsv,lifetime_gppo",
         "Correlation",
         0.9647748973569691
        ],
        [
         "Column",
         "lifetime_gsv",
         "Maximum",
         1274423
        ],
        [
         "Column",
         "lifetime_gsv",
         "Minimum",
         -2720
        ],
        [
         "Column",
         "lifetime_gppo",
         "Maximum",
         186804.4792057906
        ],
        [
         "Column",
         "lifetime_gppo",
         "Minimum",
         -47596.31383520645
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "entity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "instance",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df.select(test_cols)) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Correlation(\"lifetime_gsv\", \"lifetime_gppo\")) \\\n",
    "                    .addAnalyzer(Completeness(\"user_id\")) \\\n",
    "                    .addAnalyzer(MinLength(\"user_id\")) \\\n",
    "                    .addAnalyzer(MaxLength(\"user_id\")) \\\n",
    "                    .addAnalyzer(Maximum(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Maximum(\"lifetime_gppo\")) \\\n",
    "                    .addAnalyzer(Minimum(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Minimum(\"lifetime_gppo\")) \\\n",
    "                    .addAnalyzer(Distinctness(\"lifetime_order_count\")) \\\n",
    "                    .addAnalyzer(Distinctness(\"user_id\")) \\\n",
    "                    .addAnalyzer(StandardDeviation(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Sum(\"lifetime_gsv\")) \\\n",
    "                    .addAnalyzer(Compliance(\"top users\", \"lifetime_gppo >= 0\")) \\\n",
    "                    .addAnalyzer(ApproxCountDistinct(\"user_id\")) \\\n",
    "                    .useRepository(repository) \\\n",
    "                    .saveOrAppendResult(resultKey) \\\n",
    "                    .run()\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8994f98-653e-480a-ba42-dc5047da91a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Load the above metrics from `metric_file_path`**\n",
    "\n",
    "We can load metrics with all tags or some specific tags, we can also specify timestamp while fetching the dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "299759dd-d893-4232-9f6c-cf418bbe53f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>entity</th><th>instance</th><th>name</th><th>value</th><th>dataset_date</th><th>tag</th></tr></thead><tbody><tr><td>Column</td><td>user_id</td><td>Distinctness</td><td>1.0</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>user_id</td><td>ApproxCountDistinct</td><td>2.2857264E7</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>top users</td><td>Compliance</td><td>0.60715486774642</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>user_id</td><td>Completeness</td><td>1.0</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>user_id</td><td>MinLength</td><td>36.0</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>user_id</td><td>MaxLength</td><td>36.0</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>StandardDeviation</td><td>9756.243642500609</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Sum</td><td>7.51354820868327E10</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Dataset</td><td>*</td><td>Size</td><td>2.0832782E7</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_order_count</td><td>Distinctness</td><td>3.403290064668272E-5</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Multicolumn</td><td>lifetime_gsv,lifetime_gppo</td><td>Correlation</td><td>0.9647748973569691</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Maximum</td><td>1274423.0</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_gsv</td><td>Minimum</td><td>-2720.0</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_gppo</td><td>Maximum</td><td>186804.4792057906</td><td>1733472446745</td><td>gp-data-test</td></tr><tr><td>Column</td><td>lifetime_gppo</td><td>Minimum</td><td>-47596.31383520645</td><td>1733472446745</td><td>gp-data-test</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Column",
         "user_id",
         "Distinctness",
         1,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "user_id",
         "ApproxCountDistinct",
         22857264,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "top users",
         "Compliance",
         0.60715486774642,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "user_id",
         "Completeness",
         1,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "user_id",
         "MinLength",
         36,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "user_id",
         "MaxLength",
         36,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_gsv",
         "StandardDeviation",
         9756.243642500609,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_gsv",
         "Sum",
         75135482086.8327,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Dataset",
         "*",
         "Size",
         20832782,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_order_count",
         "Distinctness",
         0.00003403290064668272,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Multicolumn",
         "lifetime_gsv,lifetime_gppo",
         "Correlation",
         0.9647748973569691,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_gsv",
         "Maximum",
         1274423,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_gsv",
         "Minimum",
         -2720,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_gppo",
         "Maximum",
         186804.4792057906,
         1733472446745,
         "gp-data-test"
        ],
        [
         "Column",
         "lifetime_gppo",
         "Minimum",
         -47596.31383520645,
         1733472446745,
         "gp-data-test"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "entity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "instance",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "dataset_date",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "tag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = repository.load() \\\n",
    "                            .before(ResultKey.current_milli_time()) \\\n",
    "                            .withTagValues(key_tags) \\\n",
    "                            .getSuccessMetricsAsDataFrame()\n",
    "\n",
    "metrics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "454286bd-af0c-4e8a-97c1-5869da7bdc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "It helps in checking how much change we expect in certain metrics of our data wrt yesterday data. Available functionalities can be found here [anomaly_detection](https://github.com/awslabs/python-deequ/blob/48ed4420aa648a71ea34c75d39d0cc829f98abda/docs/anomaly_detection.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d482ed-98d6-4006-a0bd-a3cb6f51b2aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pydeequ.anomaly_detection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bdcbcc-7009-4767-8947-248a18c786e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20832782, 20935007)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_df = spark.sql(\"SELECT * from {table1} where current_date = '2024-12-01'\")\n",
    "\n",
    "curr_df = spark.sql(\"SELECT * from {table2} where current_date = '2024-12-02'\")\n",
    "\n",
    "prev_df.count(), curr_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5692b4-0eb8-4b22-946a-c39984d71fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004906929856991735"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20935007 - 20832782)/20832782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56012412-0dbe-4e1a-963c-e8dc6b26c266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pydeequ.repository.InMemoryMetricsRepository at 0xffff085df070>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydeequ.repository import *\n",
    "from pydeequ.verification import *\n",
    "metricsRepository = InMemoryMetricsRepository(spark)\n",
    "metricsRepository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9261b2-6c80-4809-8dda-e9ffba342c9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prevKey = ResultKey(spark, ResultKey.current_milli_time() - 24 * 60 * 60 * 1000)\n",
    "\n",
    "# maxRateIncrease -> allowable increase in size metric (2.0 -> 200% increase max)\n",
    "prev_Result = VerificationSuite(spark).onData(prev_df) \\\n",
    "    .useRepository(metricsRepository) \\\n",
    "    .saveOrAppendResult(prevKey) \\\n",
    "    .addAnomalyCheck(RelativeRateOfChangeStrategy(maxRateIncrease=0.0003), Size()) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bb3dce-4da6-4788-ba98-80e3938fd97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "currKey = ResultKey(spark, ResultKey.current_milli_time())\n",
    "\n",
    "curr_Result = VerificationSuite(spark).onData(curr_df) \\\n",
    "    .useRepository(metricsRepository) \\\n",
    "    .saveOrAppendResult(currKey) \\\n",
    "    .addAnomalyCheck(RelativeRateOfChangeStrategy( maxRateIncrease=1.0), Size()) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3d94c2d-0f78-4e62-af72-882ec707b9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly detected in the Size() metric!\n",
      "+-------+--------+----+-----------+-------------+\n",
      "| entity|instance|name|      value| dataset_date|\n",
      "+-------+--------+----+-----------+-------------+\n",
      "|Dataset|       *|Size|2.0935007E7|1733474457310|\n",
      "|Dataset|       *|Size|2.0832782E7|1733388055998|\n",
      "+-------+--------+----+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if (curr_Result.status != \"Success\"):\n",
    "    print(\"Anomaly detected in the Size() metric!\")\n",
    "    metricsRepository.load().forAnalyzers([Size()]).getSuccessMetricsAsDataFrame().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1182c3eb-c3b5-47c8-a8fb-01e053881f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Warning'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_Result.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9558953e-ccba-4a22-a128-a01b246cf4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**More details and definition of deequ methods can be found here [Automating large-scale data quality verification](https://www.vldb.org/pvldb/vol11/p1781-schelter.pdf)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "403fbf08-03df-47c5-85e7-1fcaaec3fdce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ce6c99-1cc9-4936-8363-5ec181656706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4c9cd5-83ed-4211-b0e4-f252aeaba325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Deequ - For Data Integrity and Profiling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
